<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jiacheng Zhang - Personal Website</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Caveat+Brush&family=DM+Sans:ital,wght@0,400;0,500;0,600;1,400&family=Source+Serif+4:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">
    <main>
      <div class="header">
        <div class="profile-column">
          <img src="./assets/images/profile_photo.jpg" alt="Jiacheng Zhang photo" class="profile-photo" />
          <div class="personal-links-row">
            <a href="https://www.linkedin.com/in/jiacheng-zhang-689b8319a/" class="personal-link-bar" title="LinkedIn">
              <span class="icon">in</span>
            </a>
            <a href="https://scholar.google.com/citations?user=264oplYAAAAJ&hl=en" class="personal-link-bar" title="Google Scholar">
              <span class="icon">üéì</span>
            </a>
            <a href="./assets/CV_Jiacheng_Zhang.pdf" class="personal-link-bar" title="CV">
              <span class="icon">CV</span>
            </a>
          </div>
        </div>
        <div>
          <h1>Jiacheng Zhang <span class="chinese-name">Âº†‰Ω≥Á®ã</span></h1>
          <div class="about-me">
            <p>Hi! I am a third-year PhD student at the <a href="https://umich.edu/" class="highlight-blue-yellow">University of Michigan, Ann Arbor</a> advised by <a href="https://from.so/Steve_Oney/" class="highlight-person-underline">Professor Steve Oney</a>. 
            <p>I earned my Bachelor's degree in Computer Science with <em>Summa Cum Laude</em> from <a href="https://umich.edu/" class="highlight-stroke highlight-institution">University of Michigan</a>. 
            I also obtained another degree in Electrical and Computer Engineering from <a href="https://en.sjtu.edu.cn/" class="highlight-stroke highlight-institution">Shanghai Jiao Tong University</a>. 
            During my undergraduate years, I was advised by <a href="https://andrewowens.com/" class="highlight-person-underline">Prof. Andrew Owens</a> in <span class="highlight-interest-secondary">multimodal learning</span> and <span class="highlight-interest-secondary">computer vision</span>.</p>
          </div>
        </div>
      </div>

      <section class="recent-updates-section">
        <div class="section-title">Recent Updates</div>
        <div class="updates-scroll-container">
          <div class="update-item">
            <span class="update-date">2026/02</span>
            <span>Will be joining <a href="https://www.microsoft.com/en-us/research/" class="highlight-stroke highlight-institution">Microsoft Research</a> as a research intern this summer!</span>
          </div>
          <div class="update-item">
            <span class="update-date">2025/08</span>
            <span">Finished my research intern at <a href="https://www.bosch-ai.com/" class="highlight-stroke highlight-institution">Bosch AI</a><img src="./assets/images/bosch.png" alt="Bosch" class="company-logo"> !</span>
          </div>
          <div class="update-item">
            <span class="update-date">2025/07</span>
            <span><strong>Multi-Click</strong> was accepted to UIST 2025, see you in Busan!</span>
          </div>
        </div>
      </section>

      <section class="research-section">
        <div class="section-title">Research</div>
        <p class="research-description">My research aims to improve <span class="highlight-interest">Human-AI interaction</span> by identifying and addressing usability 
        and conceptual gaps between users and the AI systems they interact with. 
        My recent research explores <span class="highlight-interest">AI-assisted web agents</span> that collaborate with users to handle online tasks such as 
        <span class="highlight-interest-secondary">multi-target manipulation</span>, <span class="highlight-interest-secondary">information retrieval</span>, and <span class="highlight-interest-secondary">decision-making</span>.</p>
        
        <div class="research-item">
          <div class="research-image">
            <video src="./assets/images/MultiClick_demo.mov" alt="WebMemo" autoplay loop muted playsinline></video>
          </div>
          <div class="research-content">
            <h3>Multi-Click: Cross-Tab Web Automation via Action Generalization            </h3>
            <div class="research-authors">
              <strong>Jiacheng Zhang</strong>, Jiawen Li, Maryam Arab, Steve Oney
            </div>
            <div class="research-venue">
              <em>UIST 2025</em>
            </div>
            <p>Multi-Click enables users to simultaneously perform the
              same action (e.g., clicking or typing) across multiple pages while
              maintaining the immediacy and understandability of direct manipulation.</p>
            <div class="research-links">
              <a href="./assets/papers/Multiclick.pdf">üìÑ paper</a>
            </div>
          </div>
        </div>


        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/webmemo.jpeg" alt="WebMemo" />
          </div>
          <div class="research-content">
            <h3>WebMemo: A Mixed-Initiative System for Extracting and Structuring Web Content</h3>
            <div class="research-authors">
              <strong>Jiacheng Zhang</strong>, Chen Yang, Eytan Adar, Steve Oney
            </div>
            <div class="research-venue">
              <em>In Submission</em>
            </div>
            <p>Through proactive and flexible data collection based on high-level user input, WebMemo reduces the cognitive load and manual effort required for managing web content.</p>
            <div class="research-links">
              <a href="./assets/papers/WebMemo.pdf">üìÑ paper</a>
            </div>
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/study.png" alt="Understanding Challenges and Needs of Using AI in Web Automation Systems" />
          </div>
          <div class="research-content">
            <h3>Understanding Challenges and Needs of Using AI in Web Automation Systems</h3>
            <div class="research-authors">
              <strong>Jiacheng Zhang</strong>, Carl Fan, Steve Oney
            </div>
            <div class="research-venue">
              <em>CHI Computational UI Workshop</em> 2025
            </div>
            <p>Understanding the challenges and needs of using AI in web automation systems through user studies and analysis.</p>
            <div class="research-links">
              <a href="https://drive.google.com/file/d/1K0n4PgYUPEutkuwFtBZNIZxS4GG-zS1L/view">üìÑ paper</a>
            </div>
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/edbooks.png" alt="EDBooks" />
          </div>
          <div class="research-content">
            <h3>EDBooks: AI-Enhanced Interactive Narratives for Programming Education</h3>
            <div class="research-authors">
              Steve Oney, Yue Shen, Fei Wu, Young Suh Hong, Ziang Wang, Yamini Khajekar, <strong>Jiacheng Zhang</strong>, April Yi Wang
            </div>
            <div class="research-venue">
              <em>arXiv</em> 2024
            </div>
            <p>AI-enhanced interactive narratives for programming education.</p>
            <div class="research-links">
              <a href="https://arxiv.org/pdf/2411.10687">üìÑ paper</a>
            </div>
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/touch2im.png" alt="Generating Visual Scenes from Touch" />
          </div>
          <div class="research-content">
            <h3>Generating Visual Scenes from Touch</h3>
            <div class="research-authors">
              Fengyu Yang, <strong>Jiacheng Zhang</strong>, Andrew Owens
            </div>
            <div class="research-venue">
              <em>ICCV</em> 2023
            </div>
            <p>We use diffusion to generate images from a touch signal (and vice versa).</p>
            <div class="research-links">
              <a href="https://fredfyyang.github.io/vision-from-touch/">üåê project page</a>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf">üìÑ paper</a>
            </div>
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/touch-go.png" alt="Touch and Go" />
          </div>
          <div class="research-content">
            <h3>Touch and Go: Learning from Human-Collected Vision and Touch</h3>
            <div class="research-authors">
              Fengyu Yang*, Chenyang Ma*, <strong>Jiacheng Zhang</strong>, Jing Zhu, Wenzhen Yuan, Andrew Owens
            </div>
            <div class="research-venue">
              <em>NeurIPS</em> 2022
            </div>
            <p>A dataset of paired vision-and-touch data collected by humans. We apply it to: 1) restyling an image to match a tactile input, 2) self-supervised representation learning, 3) multimodal video prediction.</p>
            <div class="research-links">
              <a href="https://touch-and-go.github.io/">üåê project page</a>
              <a href="https://openreview.net/pdf?id=ZZ3FeSSPPblo">üìÑ paper</a>
              <a href="https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B">üíæ dataset</a>
            </div>
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <img src="./assets/images/miwa.jpg" alt="MIWA" />
          </div>
          <div class="research-content">
            <h3>MIWA: Mixed-Initiative Web Automation for Better User Control and Confidence</h3>
            <div class="research-authors">
              Weihao Chen, Xiaoyu Liu, <strong>Jiacheng Zhang</strong>, Zhicheng Huang, Ian Long Lam, Rui Dong, Xinyu Wang, Tianyi Zhang
            </div>
            <div class="research-venue">
              <em>UIST</em> 2023
            </div>
            <p>We provide MIWA, a mixed-initiative web automation system that enables users to create web scraping programs by demonstrating what contents they want from the targeted websites.</p>
            <div class="research-links">
              <a href="https://tianyi-zhang.github.io/files/uist2023-miwa.pdf">üìÑ paper</a>
            </div>
          </div>
        </div>
      </section>

      <section class="misc-section">
        <div class="section-title">Misc</div>
        <ul class="misc-list">
          <li>Britpop enthusiast ‚Äî Oasis, Blur, Coldplay</li>
          <li>Dota 2 when not researching</li>
        </ul>
      </section>
    </main>
  </div>

  <footer class="copyright-footer">
    <p>&copy; 2025 Jiacheng Zhang.</p>
  </footer>
</body>
</html>
